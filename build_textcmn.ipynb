{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zangsir/Desktop/text_cmn\n"
     ]
    }
   ],
   "source": [
    "cd ~/Desktop/text_cmn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def concate_files(output_file,filenames,path):\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for fname in filenames:\n",
    "            with open(path+fname) as infile:\n",
    "                outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['DIL', 'DOH', 'SUC', 'RUO', 'WAJ', 'XIH', 'CHJ', 'TIK', 'XIN', 'KOF', 'FAJ', 'SHH', 'XUL', 'LIS', 'CHX', 'XIY', 'MAK', 'HAT', 'XIJ', 'OUT'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path='data/'\n",
    "\n",
    "#all txt concatenate for a single speaker\n",
    "all_txt=[f for f in listdir(path) if f.endswith('.txt')]\n",
    "all_speaker=set([s[:3] for s in all_txt])\n",
    "print all_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for spk in all_speaker:\n",
    "    spk_txts=[f for f in all_txt if f.startswith(spk)]\n",
    "    output_file=spk+'_all.txt'\n",
    "    concate_files(output_file,spk_txts,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_data(suffix,path):\n",
    "    all_txt=[f for f in listdir(path) if f.endswith(suffix)]\n",
    "    all_speaker=set([s[:3] for s in all_txt])\n",
    "    print all_speaker\n",
    "    for spk in all_speaker:\n",
    "        spk_txts=[f for f in all_txt if f.startswith(spk)]\n",
    "        output_file=spk+'_all'+suffix\n",
    "        concate_files(output_file,spk_txts,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHJ_all.txt        KOF_all.txt        SUC_all.txt        XIY_all.txt\r\n",
      "CHX_all.txt        LIS_all.txt        TIK_all.txt        XUL_all.txt\r\n",
      "DIL_all.txt        MAK_all.txt        WAJ_all.txt        build_text_cmn.py\r\n",
      "DOH_all.txt        OUT_all.txt        XIH_all.txt        \u001b[1m\u001b[34mdata\u001b[m\u001b[m/\r\n",
      "FAJ_all.txt        RUO_all.txt        XIJ_all.txt\r\n",
      "HAT_all.txt        SHH_all.txt        XIN_all.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flist=[f for f in listdir('/Users/zangsir/Desktop/text_cmn/') if f.endswith('all.txt')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHJ_all.txt',\n",
       " 'CHX_all.txt',\n",
       " 'DIL_all.txt',\n",
       " 'DOH_all.txt',\n",
       " 'FAJ_all.txt',\n",
       " 'HAT_all.txt',\n",
       " 'KOF_all.txt',\n",
       " 'LIS_all.txt',\n",
       " 'MAK_all.txt',\n",
       " 'OUT_all.txt',\n",
       " 'RUO_all.txt',\n",
       " 'SHH_all.txt',\n",
       " 'SUC_all.txt',\n",
       " 'TIK_all.txt',\n",
       " 'WAJ_all.txt',\n",
       " 'XIH_all.txt',\n",
       " 'XIJ_all.txt',\n",
       " 'XIN_all.txt',\n",
       " 'XIY_all.txt',\n",
       " 'XUL_all.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_cmn/CHJ_all.txt\n",
      "text_cmn/CHX_all.txt\n",
      "text_cmn/DIL_all.txt\n",
      "text_cmn/DOH_all.txt\n",
      "text_cmn/FAJ_all.txt\n",
      "text_cmn/HAT_all.txt\n",
      "text_cmn/KOF_all.txt\n",
      "text_cmn/LIS_all.txt\n",
      "text_cmn/MAK_all.txt\n",
      "text_cmn/OUT_all.txt\n",
      "text_cmn/RUO_all.txt\n",
      "text_cmn/SHH_all.txt\n",
      "text_cmn/SUC_all.txt\n",
      "text_cmn/TIK_all.txt\n",
      "text_cmn/WAJ_all.txt\n",
      "text_cmn/XIH_all.txt\n",
      "text_cmn/XIJ_all.txt\n",
      "text_cmn/XIN_all.txt\n",
      "text_cmn/XIY_all.txt\n",
      "text_cmn/XUL_all.txt\n"
     ]
    }
   ],
   "source": [
    "for f in flist:\n",
    "    print 'text_cmn/'+f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# mapping between file name (CHJ00001) and sentID (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def map_fname_sentID(suffix,path):\n",
    "    all_txt=[f for f in listdir(path) if f.endswith(suffix)]\n",
    "    all_speaker=set([s[:3] for s in all_txt])\n",
    "    print all_speaker\n",
    "    global_dict={}\n",
    "    \n",
    "    for spk in all_speaker:\n",
    "        sent_id=1\n",
    "        local_dict={}\n",
    "        spk_txts=[f for f in all_txt if f.startswith(spk)]\n",
    "        for i in range(len(spk_txts)):\n",
    "            local_dict[spk_txts[i]]=sent_id\n",
    "            sent_id+=1\n",
    "        global_dict[spk]=local_dict\n",
    "    return global_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['DIL', 'DOH', 'SUC', 'RUO', 'WAJ', 'XIH', 'CHJ', 'TIK', 'XIN', 'KOF', 'FAJ', 'SHH', 'XUL', 'LIS', 'CHX', 'XIY', 'MAK', 'HAT', 'XIJ', 'OUT'])\n"
     ]
    }
   ],
   "source": [
    "path='text_cmn/data/'\n",
    "suffix='.txt'\n",
    "mapping=map_fname_sentID(suffix,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(mapping,open('fname_ID_map.pkl','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m=pickle.load(open('fname_ID_map.pkl','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CHJ000001.txt': 1,\n",
       " 'CHJ000002.txt': 2,\n",
       " 'CHJ000003.txt': 3,\n",
       " 'CHJ000004.txt': 4,\n",
       " 'CHJ000005.txt': 5,\n",
       " 'CHJ000006.txt': 6,\n",
       " 'CHJ000008.txt': 7,\n",
       " 'CHJ000009.txt': 8,\n",
       " 'CHJ000010.txt': 9,\n",
       " 'CHJ000011.txt': 10,\n",
       " 'CHJ000012.txt': 11,\n",
       " 'CHJ000013.txt': 12,\n",
       " 'CHJ000014.txt': 13,\n",
       " 'CHJ000015.txt': 14,\n",
       " 'CHJ000016.txt': 15,\n",
       " 'CHJ000017.txt': 16,\n",
       " 'CHJ000018.txt': 17,\n",
       " 'CHJ000019.txt': 18,\n",
       " 'CHJ000020.txt': 19,\n",
       " 'CHJ000021.txt': 20,\n",
       " 'CHJ000022.txt': 21,\n",
       " 'CHJ000023.txt': 22,\n",
       " 'CHJ000024.txt': 23,\n",
       " 'CHJ000025.txt': 24,\n",
       " 'CHJ000026.txt': 25,\n",
       " 'CHJ000027.txt': 26,\n",
       " 'CHJ000028.txt': 27,\n",
       " 'CHJ000029.txt': 28,\n",
       " 'CHJ000030.txt': 29,\n",
       " 'CHJ000031.txt': 30,\n",
       " 'CHJ000032.txt': 31,\n",
       " 'CHJ000033.txt': 32,\n",
       " 'CHJ000034.txt': 33,\n",
       " 'CHJ000035.txt': 34,\n",
       " 'CHJ000036.txt': 35,\n",
       " 'CHJ000037.txt': 36,\n",
       " 'CHJ000038.txt': 37,\n",
       " 'CHJ000039.txt': 38,\n",
       " 'CHJ000040.txt': 39,\n",
       " 'CHJ000041.txt': 40,\n",
       " 'CHJ000042.txt': 41,\n",
       " 'CHJ000043.txt': 42,\n",
       " 'CHJ000044.txt': 43,\n",
       " 'CHJ000045.txt': 44,\n",
       " 'CHJ000046.txt': 45,\n",
       " 'CHJ000047.txt': 46,\n",
       " 'CHJ000048.txt': 47,\n",
       " 'CHJ000049.txt': 48,\n",
       " 'CHJ000050.txt': 49,\n",
       " 'CHJ000051.txt': 50,\n",
       " 'CHJ000052.txt': 51,\n",
       " 'CHJ000053.txt': 52,\n",
       " 'CHJ000054.txt': 53,\n",
       " 'CHJ000055.txt': 54,\n",
       " 'CHJ000056.txt': 55,\n",
       " 'CHJ000057.txt': 56,\n",
       " 'CHJ000058.txt': 57,\n",
       " 'CHJ000059.txt': 58,\n",
       " 'CHJ000060.txt': 59,\n",
       " 'CHJ000061.txt': 60,\n",
       " 'CHJ000062.txt': 61,\n",
       " 'CHJ000063.txt': 62,\n",
       " 'CHJ000064.txt': 63,\n",
       " 'CHJ000065.txt': 64,\n",
       " 'CHJ000066.txt': 65,\n",
       " 'CHJ000067.txt': 66,\n",
       " 'CHJ000068.txt': 67,\n",
       " 'CHJ000069.txt': 68,\n",
       " 'CHJ000070.txt': 69,\n",
       " 'CHJ000071.txt': 70,\n",
       " 'CHJ000072.txt': 71,\n",
       " 'CHJ000073.txt': 72,\n",
       " 'CHJ000074.txt': 73,\n",
       " 'CHJ000075.txt': 74,\n",
       " 'CHJ000076.txt': 75,\n",
       " 'CHJ000077.txt': 76,\n",
       " 'CHJ000078.txt': 77,\n",
       " 'CHJ000079.txt': 78,\n",
       " 'CHJ000080.txt': 79,\n",
       " 'CHJ000082.txt': 80,\n",
       " 'CHJ000083.txt': 81,\n",
       " 'CHJ000084.txt': 82,\n",
       " 'CHJ000085.txt': 83,\n",
       " 'CHJ000086.txt': 84,\n",
       " 'CHJ000087.txt': 85,\n",
       " 'CHJ000088.txt': 86,\n",
       " 'CHJ000089.txt': 87,\n",
       " 'CHJ000090.txt': 88,\n",
       " 'CHJ000091.txt': 89,\n",
       " 'CHJ000092.txt': 90,\n",
       " 'CHJ000093.txt': 91,\n",
       " 'CHJ000094.txt': 92,\n",
       " 'CHJ000095.txt': 93,\n",
       " 'CHJ000096.txt': 94,\n",
       " 'CHJ000097.txt': 95,\n",
       " 'CHJ000098.txt': 96,\n",
       " 'CHJ000099.txt': 97,\n",
       " 'CHJ000100.txt': 98,\n",
       " 'CHJ000101.txt': 99,\n",
       " 'CHJ000102.txt': 100,\n",
       " 'CHJ000103.txt': 101,\n",
       " 'CHJ000104.txt': 102,\n",
       " 'CHJ000105.txt': 103,\n",
       " 'CHJ000106.txt': 104,\n",
       " 'CHJ000107.txt': 105,\n",
       " 'CHJ000108.txt': 106,\n",
       " 'CHJ000109.txt': 107,\n",
       " 'CHJ000110.txt': 108,\n",
       " 'CHJ000111.txt': 109,\n",
       " 'CHJ000112.txt': 110,\n",
       " 'CHJ000113.txt': 111,\n",
       " 'CHJ000114.txt': 112,\n",
       " 'CHJ000115.txt': 113,\n",
       " 'CHJ000116.txt': 114,\n",
       " 'CHJ000117.txt': 115,\n",
       " 'CHJ000118.txt': 116,\n",
       " 'CHJ000119.txt': 117,\n",
       " 'CHJ000120.txt': 118,\n",
       " 'CHJ000121.txt': 119,\n",
       " 'CHJ000122.txt': 120,\n",
       " 'CHJ000123.txt': 121,\n",
       " 'CHJ000124.txt': 122,\n",
       " 'CHJ000125.txt': 123,\n",
       " 'CHJ000126.txt': 124,\n",
       " 'CHJ000127.txt': 125,\n",
       " 'CHJ000128.txt': 126,\n",
       " 'CHJ000129.txt': 127,\n",
       " 'CHJ000130.txt': 128,\n",
       " 'CHJ000131.txt': 129,\n",
       " 'CHJ000132.txt': 130,\n",
       " 'CHJ000133.txt': 131,\n",
       " 'CHJ000134.txt': 132,\n",
       " 'CHJ000135.txt': 133,\n",
       " 'CHJ000136.txt': 134,\n",
       " 'CHJ000137.txt': 135,\n",
       " 'CHJ000138.txt': 136,\n",
       " 'CHJ000139.txt': 137,\n",
       " 'CHJ000140.txt': 138,\n",
       " 'CHJ000141.txt': 139,\n",
       " 'CHJ000142.txt': 140,\n",
       " 'CHJ000143.txt': 141,\n",
       " 'CHJ000144.txt': 142,\n",
       " 'CHJ000145.txt': 143,\n",
       " 'CHJ000146.txt': 144,\n",
       " 'CHJ000147.txt': 145,\n",
       " 'CHJ000148.txt': 146,\n",
       " 'CHJ000149.txt': 147,\n",
       " 'CHJ000150.txt': 148,\n",
       " 'CHJ000151.txt': 149,\n",
       " 'CHJ000152.txt': 150,\n",
       " 'CHJ000153.txt': 151,\n",
       " 'CHJ000154.txt': 152,\n",
       " 'CHJ000155.txt': 153,\n",
       " 'CHJ000156.txt': 154,\n",
       " 'CHJ000157.txt': 155,\n",
       " 'CHJ000158.txt': 156,\n",
       " 'CHJ000159.txt': 157,\n",
       " 'CHJ000160.txt': 158,\n",
       " 'CHJ000161.txt': 159,\n",
       " 'CHJ000162.txt': 160,\n",
       " 'CHJ000164.txt': 161,\n",
       " 'CHJ000165.txt': 162,\n",
       " 'CHJ000166.txt': 163,\n",
       " 'CHJ000167.txt': 164,\n",
       " 'CHJ000168.txt': 165,\n",
       " 'CHJ000169.txt': 166,\n",
       " 'CHJ000170.txt': 167,\n",
       " 'CHJ000171.txt': 168,\n",
       " 'CHJ000172.txt': 169,\n",
       " 'CHJ000174.txt': 170,\n",
       " 'CHJ000175.txt': 171,\n",
       " 'CHJ000176.txt': 172,\n",
       " 'CHJ000177.txt': 173,\n",
       " 'CHJ000178.txt': 174,\n",
       " 'CHJ000179.txt': 175,\n",
       " 'CHJ000180.txt': 176,\n",
       " 'CHJ000181.txt': 177,\n",
       " 'CHJ000182.txt': 178,\n",
       " 'CHJ000183.txt': 179,\n",
       " 'CHJ000184.txt': 180,\n",
       " 'CHJ000185.txt': 181,\n",
       " 'CHJ000186.txt': 182,\n",
       " 'CHJ000187.txt': 183,\n",
       " 'CHJ000188.txt': 184,\n",
       " 'CHJ000189.txt': 185,\n",
       " 'CHJ000190.txt': 186,\n",
       " 'CHJ000191.txt': 187,\n",
       " 'CHJ000192.txt': 188,\n",
       " 'CHJ000193.txt': 189,\n",
       " 'CHJ000195.txt': 190,\n",
       " 'CHJ000196.txt': 191,\n",
       " 'CHJ000197.txt': 192,\n",
       " 'CHJ000198.txt': 193,\n",
       " 'CHJ000199.txt': 194,\n",
       " 'CHJ000200.txt': 195,\n",
       " 'CHJ000201.txt': 196,\n",
       " 'CHJ000202.txt': 197,\n",
       " 'CHJ000203.txt': 198,\n",
       " 'CHJ000204.txt': 199,\n",
       " 'CHJ000205.txt': 200,\n",
       " 'CHJ000206.txt': 201,\n",
       " 'CHJ000207.txt': 202,\n",
       " 'CHJ000208.txt': 203,\n",
       " 'CHJ000209.txt': 204,\n",
       " 'CHJ000210.txt': 205,\n",
       " 'CHJ000211.txt': 206,\n",
       " 'CHJ000212.txt': 207,\n",
       " 'CHJ000213.txt': 208,\n",
       " 'CHJ000214.txt': 209,\n",
       " 'CHJ000215.txt': 210,\n",
       " 'CHJ000216.txt': 211,\n",
       " 'CHJ000217.txt': 212,\n",
       " 'CHJ000218.txt': 213,\n",
       " 'CHJ000219.txt': 214,\n",
       " 'CHJ000220.txt': 215,\n",
       " 'CHJ000221.txt': 216,\n",
       " 'CHJ000223.txt': 217,\n",
       " 'CHJ000224.txt': 218,\n",
       " 'CHJ000225.txt': 219,\n",
       " 'CHJ000226.txt': 220,\n",
       " 'CHJ000227.txt': 221,\n",
       " 'CHJ000228.txt': 222,\n",
       " 'CHJ000229.txt': 223,\n",
       " 'CHJ000230.txt': 224,\n",
       " 'CHJ000231.txt': 225,\n",
       " 'CHJ000232.txt': 226,\n",
       " 'CHJ000233.txt': 227,\n",
       " 'CHJ000234.txt': 228,\n",
       " 'CHJ000235.txt': 229,\n",
       " 'CHJ000236.txt': 230,\n",
       " 'CHJ000237.txt': 231,\n",
       " 'CHJ000238.txt': 232,\n",
       " 'CHJ000239.txt': 233,\n",
       " 'CHJ000240.txt': 234,\n",
       " 'CHJ000241.txt': 235,\n",
       " 'CHJ000242.txt': 236,\n",
       " 'CHJ000243.txt': 237,\n",
       " 'CHJ000244.txt': 238,\n",
       " 'CHJ000245.txt': 239,\n",
       " 'CHJ000246.txt': 240,\n",
       " 'CHJ000247.txt': 241,\n",
       " 'CHJ000248.txt': 242,\n",
       " 'CHJ000249.txt': 243,\n",
       " 'CHJ000250.txt': 244,\n",
       " 'CHJ000251.txt': 245,\n",
       " 'CHJ000252.txt': 246,\n",
       " 'CHJ000254.txt': 247,\n",
       " 'CHJ000255.txt': 248,\n",
       " 'CHJ000256.txt': 249,\n",
       " 'CHJ000257.txt': 250,\n",
       " 'CHJ000258.txt': 251,\n",
       " 'CHJ000259.txt': 252,\n",
       " 'CHJ000260.txt': 253,\n",
       " 'CHJ000263.txt': 254,\n",
       " 'CHJ000264.txt': 255,\n",
       " 'CHJ000265.txt': 256,\n",
       " 'CHJ000266.txt': 257,\n",
       " 'CHJ000267.txt': 258,\n",
       " 'CHJ000268.txt': 259,\n",
       " 'CHJ000269.txt': 260,\n",
       " 'CHJ000270.txt': 261}"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m['CHJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map between syl_id and tok_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 0, 2: 1, 3: 2, 4: 2, 5: 3, 6: 4, 7: 4, 8: 5, 9: 5, 10: 6, 11: 6, 12: 6, 13: 7, 14: 8, 15: 8, 16: 9, 17: 10, 18: 10}\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "#after you found the sentence file (e.g., CHJ00008.txt) you build this map on the fly, \n",
    "#so syl 12_13_14 can map to tok id 6,7,8, then you use the map to find the corresponding lines in conll.\n",
    "def map_syl_tok(tokenized_sent):\n",
    "    syl2tok={}\n",
    "    syl_id=0\n",
    "    tokenized_sent=codecs.decode(tokenized_sent,'utf-8')\n",
    "    toks=tokenized_sent.split(' ')\n",
    "    stripped=tokenized_sent.replace(' ','').replace('\\n','')\n",
    "    #print stripped\n",
    "    num_syl=len(stripped)\n",
    "    #print num_syl\n",
    "    #print toks\n",
    "    for i in range(len(toks)):\n",
    "        tok_id=i\n",
    "        tok=toks[i]\n",
    "        #print tok\n",
    "        if tok=='' or tok=='\\n':\n",
    "            #print 'skip'\n",
    "            continue\n",
    "        for char in tok:\n",
    "            #print char\n",
    "            syl2tok[syl_id]=i\n",
    "            syl_id+=1\n",
    "            if syl_id>num_syl:\n",
    "                break    \n",
    "    return syl2tok\n",
    "            \n",
    "s='中国 对 美国 的 贸易 顺差 差不多 有 四百 亿 美元 \\\n",
    "'\n",
    "print map_syl_tok(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build the pipeline: from a line of ngram to feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram='-0.405,-0.392,-0.379,-0.366,-0.354,-0.341,-0.328,-0.315,-0.302,-0.289,-0.276,-0.264,-0.251,-0.238,-0.225,-0.212,-0.199,-0.198,-0.196,-0.193,-0.189,-0.187,-0.186,-0.184,-0.181,-0.176,-0.169,-0.165,-0.161,-0.159,-0.156,-0.153,-0.15,-0.146,-0.143,-0.14,-0.137,-0.135,-0.134,-0.135,-0.135,-0.134,-0.132,-0.129,-0.127,-0.126,-0.128,-0.13,-0.132,-0.133,-0.132,-0.132,-0.132,-0.178,-0.179,-0.179,-0.177,-0.202,-0.201,-0.199,-0.199,-0.199,-0.2,-0.201,-0.201,-0.2,-0.196,-0.222,-0.258,-0.255,-0.248,-0.283,-0.282,-0.281,-0.28,-0.315,-0.314,-0.312,-0.311,-0.343,-0.341,-0.339,-0.339,-0.338,-0.308,-0.316,-0.34,-0.364,-0.389,-0.405,-0.406,-0.407,-0.408,-0.409,-0.41,-0.412,-0.42,-0.426,-0.427,-0.427,-0.427,-0.427,-0.427,-0.427,-0.426,-0.425,-0.425,-0.424,-0.424,-0.424,-0.425,-0.425,-0.426,-0.425,-0.425,-0.424,-0.424,-0.425,-0.426,-0.428,-0.43,-0.432,-0.435,-0.435,-0.434,-0.432,-0.43,-0.43,-0.432,-0.435,-0.436,-0.436,-0.436,-0.435,-0.434,-0.433,-0.431,-0.43,-0.429,-0.429,-0.429,-0.429,-0.43,-0.432,-0.439,-0.444,-0.438,-0.417,-0.397,-0.376,-0.355,-0.328,-0.3,-0.273,-0.245,-0.218,-0.191,-0.163,-0.136,-0.108,-0.081,-0.054,-0.027,0.001,0.028,0.055,0.082,0.11,0.137,0.164,0.191,0.218,0.245,0.272,0.299,0.326,0.353,0.38,0.407,0.434,0.461,0.488,0.514,0.533,0.524,0.464,0.454,0.444,0.429,0.399,0.353,0.315,0.281,0.246,0.209,0.197,0.197,-0.207,-0.201,-0.194,134,mid_mid_mid,syl_csv_norm_whole_meta/CHJ000008_whole,12_13_14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from this ngram line, you will need to verify with with data/CHJ00008.phons about the tones of these indexes, and \n",
    "#then build a mapping from syl2tok, together with your fname_ID_map, you can pick out the specific tok lines of interest\n",
    "#for feature extraction from the conll file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHJ_all.txt        KOF_all.txt        SUC_all.txt        XIY_all.txt\r\n",
      "CHX_all.txt        LIS_all.txt        TIK_all.txt        XUL_all.txt\r\n",
      "DIL_all.txt        MAK_all.txt        WAJ_all.txt        build_text_cmn.py\r\n",
      "DOH_all.txt        OUT_all.txt        XIH_all.txt        \u001b[1m\u001b[34mdata\u001b[m\u001b[m/\r\n",
      "FAJ_all.txt        RUO_all.txt        XIJ_all.txt\r\n",
      "HAT_all.txt        SHH_all.txt        XIN_all.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls text_cmn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngram_list=ngram.split(',')\n",
    "tones=ngram_list[-4]\n",
    "fname=ngram_list[-2].split('/')[1].split('_')[0]\n",
    "syl_ids=ngram_list[-1].split('_')\n",
    "\n",
    "txt_file='text_cmn/data/'+fname+'.txt'\n",
    "phons_file='text_cmn/data/'+fname+'.phons'\n",
    "fname_ID_map=pickle.load(open('fname_ID_map.pkl','r'))\n",
    "conll_merged_file=fname[:3]+'_merge.conll'\n",
    "\n",
    "# TODO:we need a function to verify tones\n",
    "sent_id=fname_ID_map[fname[:3]][fname+'.txt']\n",
    "\n",
    "# syl to tok id\n",
    "tokenized_sentence=open(txt_file,'r').read()\n",
    "syl2tok_map=map_syl_tok(tokenized_sentence)\n",
    "tok_ids=[syl2tok_map[int(i)] for i in syl_ids]\n",
    "\n",
    "# parse conll\n",
    "cdict=parse_conll(conll_merged_file)\n",
    "\n",
    "#get to targeted lines in conll\n",
    "target_conll_sentence=cdict[sent_id]\n",
    "target_toks=[target_conll_sentence[i] for i in tok_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def parse_conll(conll_file):\n",
    "    \"\"\"returns a dictionary from sentence id to the rows of conll files belonging to that sentence\"\"\"\n",
    "    f=open(conll_file,'r').read().split('\\n')\n",
    "    cdict=defaultdict(list)\n",
    "    clist=[]\n",
    "    for line in f:\n",
    "        if line.startswith('#') or line=='':\n",
    "            continue\n",
    "        l=line.split('\\t')\n",
    "        clist.append(l)\n",
    "    for line in clist:\n",
    "        sent_id=line[0]\n",
    "        #print sent_id\n",
    "        cdict[int(sent_id)].append(line)\n",
    "    return cdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_to_conll(ngram):\n",
    "    \"\"\"taking one line of ngram file, map it onto relevant token lines in the target conll file\"\"\"\n",
    "    #for an example of input ngram, see the ngram variable defined above. it is one line from a ngram file, such as \n",
    "    #bigram file, where the line is the TS of tone ngrams and then meta attributes\n",
    "    ngram_list=ngram.split(',')\n",
    "    tones=ngram_list[-4]\n",
    "    fname=ngram_list[-2].split('/')[1].split('_')[0]\n",
    "    syl_ids=ngram_list[-1].split('_')\n",
    "    \n",
    "    merge_conll_path='merged_conll/'\n",
    "    txt_file='text_cmn/data/'+fname+'.txt'\n",
    "    phons_file='text_cmn/data/'+fname+'.phons'\n",
    "    fname_ID_map=pickle.load(open('fname_ID_map.pkl','r'))\n",
    "    conll_merged_file=merge_conll_path+fname[:3]+'_all.txt_merge.conll'\n",
    "\n",
    "    # TODO:we need a function to verify tones\n",
    "    sent_id=fname_ID_map[fname[:3]][fname+'.txt']\n",
    "\n",
    "    # syl to tok id\n",
    "    tokenized_sentence=open(txt_file,'r').read()\n",
    "    syl2tok_map=map_syl_tok(tokenized_sentence)\n",
    "    tok_ids=[syl2tok_map[int(i)] for i in syl_ids]\n",
    "\n",
    "    # parse conll\n",
    "    conll_dict=parse_conll(conll_merged_file)\n",
    "\n",
    "    #get to targeted lines in conll\n",
    "    target_conll_sentence=conll_dict[sent_id]\n",
    "    #print target_conll_sentence\n",
    "    #print 'tok_ids',tok_ids\n",
    "    target_toks=[target_conll_sentence[i] for i in tok_ids]\n",
    "    return target_toks, syl2tok_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngram='-0.029,0.013,0.055,0.097,0.139,0.137,0.132,0.123,0.088,0.083,0.08,0.078,0.076,0.074,0.072,0.068,0.065,0.061,0.057,0.053,0.05,0.047,0.044,0.04,0.038,0.035,0.034,0.033,0.033,0.033,0.035,0.037,0.041,0.046,0.054,0.063,0.071,0.077,0.081,0.084,0.086,0.087,0.087,0.087,0.085,-0.052,-0.058,-0.067,-0.078,-0.087,-0.085,-0.08,-0.075,-0.073,-0.074,-0.076,-0.08,-0.084,-0.087,-0.09,-0.093,-0.096,-0.099,-0.102,-0.105,-0.107,-0.11,-0.113,-0.116,-0.119,-0.122,-0.122,-0.122,-0.122,-0.122,-0.121,-0.189,-0.188,-0.188,-0.187,-0.186,-0.185,-0.183,-0.181,-0.178,-0.176,-0.342,-0.341,-0.339,-0.337,-0.402,-0.401,-0.4,-0.399,-0.398,-0.464,-0.464,-0.464,-0.464,-0.464,-0.465,-0.526,-0.527,-0.527,-0.528,-0.528,-0.531,-0.54,-0.542,-0.543,-0.544,-0.545,-0.549,-0.552,-0.553,-0.553,-0.553,-0.554,-0.554,-0.554,-0.555,-0.555,-0.554,-0.554,-0.553,-0.551,-0.55,-0.549,-0.515,-0.516,-0.489,-0.49,-0.492,-0.494,-0.495,-0.496,-0.496,-0.497,-0.498,-0.523,-0.527,-0.544,-0.548,-0.565,-0.571,-0.558,-0.546,-0.533,-0.521,-0.509,-0.496,-0.484,-0.471,-0.459,-0.447,-0.434,-0.422,-0.41,-0.397,-0.389,-0.389,-0.388,-0.386,-0.384,-0.383,-0.382,-0.383,-0.385,-0.389,-0.392,-0.395,-0.398,-0.401,-0.404,-0.406,-0.408,-0.41,-0.412,-0.419,-0.426,-0.43,-0.433,-0.449,-0.454,-0.458,-0.461,-0.465,-0.468,-0.471,-0.476,-0.48,-0.483,-0.485,-0.491,-0.496,-0.498,-0.501,-0.506,-0.518,-0.52,134,mid_mid_end,syl_csv_norm_whole_meta/CHJ000014_whole,2_3_4'\n",
    "ngram='-0.063,-0.037,-0.011,0.014,0.04,0.066,0.091,0.117,0.143,0.168,0.194,0.22,0.245,0.271,0.296,0.322,0.347,0.373,0.398,0.423,0.449,0.474,0.499,0.525,0.55,0.575,0.6,0.626,0.651,0.676,0.685,0.687,0.697,0.711,0.723,0.728,0.725,0.717,0.71,0.71,0.717,0.724,0.728,0.729,0.731,0.734,0.737,0.738,0.728,0.684,0.598,0.531,0.489,0.465,0.441,0.416,0.392,0.368,0.343,0.319,0.294,0.27,0.246,0.221,0.197,0.172,0.147,0.123,0.098,0.074,0.049,0.024,-0.0,-0.025,-0.05,-0.074,-0.099,-0.124,-0.148,-0.173,-0.198,-0.223,-0.248,-0.272,-0.297,-0.322,-0.334,-0.342,-0.347,-0.352,-0.355,-0.361,-0.366,-0.371,-0.377,-0.381,-0.381,-0.38,-0.377,-0.372,-0.365,-0.361,-0.359,-0.357,-0.354,-0.35,-0.345,-0.34,-0.336,-0.332,-0.328,-0.323,-0.318,-0.313,-0.308,-0.302,-0.293,-0.286,-0.276,-0.27,-0.263,-0.256,-0.253,-0.221,-0.218,-0.181,-0.18,-0.182,-0.189,-0.196,-0.198,-0.199,-0.196,-0.189,-0.181,-0.153,-0.119,-0.085,-0.052,-0.018,0.016,0.05,0.084,0.118,0.151,0.185,0.219,0.252,0.286,0.319,0.335,0.329,0.319,0.306,0.295,0.281,0.259,0.24,0.227,0.217,0.206,0.193,0.176,0.158,0.141,0.126,0.113,0.102,0.088,0.076,0.066,0.06,0.056,0.052,0.046,0.038,0.028,0.019,0.011,0.003,-0.009,-0.021,-0.031,-0.039,-0.048,-0.053,-0.055,-0.056,-0.058,-0.061,-0.042,-0.036,-0.035,-0.037,-0.049,-0.079,-0.087,-0.095,-0.107,-0.116,134,mid_mid_mid,syl_csv_norm_whole_meta/CHJ000027_whole,4_5_6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_toks = map_to_conll(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "出口\n",
      "出口\n",
      "占\n"
     ]
    }
   ],
   "source": [
    "for i in target_toks:\n",
    "    print i[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['26', '247', '4', '\\xe5\\x87\\xba\\xe5\\x8f\\xa3', '\\xe5\\x87\\xba\\xe5\\x8f\\xa3', 'NN', 'O', '5', 'nsubj', '_'], ['26', '247', '4', '\\xe5\\x87\\xba\\xe5\\x8f\\xa3', '\\xe5\\x87\\xba\\xe5\\x8f\\xa3', 'NN', 'O', '5', 'nsubj', '_'], ['26', '248', '5', '\\xe5\\x8d\\xa0', '\\xe5\\x8d\\xa0', 'VV', 'O', '0', 'ROOT', '_']]\n"
     ]
    }
   ],
   "source": [
    "print target_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now we have the target conll line we can extract feature from. notice these three lines would correspond to the one\n",
    "#trigram, so we are extracting one line of feature.(one training example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extracting features\n",
    "\n",
    "now what we have is a handy function that basically can take one line at a time from the sound domain time-series database of n-grams (such as a bigram file), and convert it into tokens in the conll file in text domain, and we can directly extract features from it. hooray!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#steps: take a ngram (bigram, e.g.) file, then process one line at a time. for each line, convert to text domain token,\n",
    "#and then extract features.\n",
    "\n",
    "#we can write a function that extracts feature (a feature extractor) from one line of ngram. \n",
    "\n",
    "#pos1,pos2,po3,func1,func2,func3,starting_pitch,ending_pitch,sent_position2, is_entity, tok_bound, \n",
    "#has_antecedent, has_anaphor, singleton\n",
    "\n",
    "\n",
    "def get_pitch_features(ngram_line):\n",
    "    ngram_list=ngram.split(',')\n",
    "    start_pitch=ngram_list[0]\n",
    "    end_pitch=ngram_list[-5]\n",
    "    return start_pitch,end_pitch\n",
    "\n",
    "def is_entity_unit(entity):\n",
    "    return 0 if entity=='O' else 1\n",
    "\n",
    "def is_entity(entity_list):\n",
    "    for ent in entity_list:\n",
    "        if is_entity_unit(ent):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def is_tok_bound(ngram_line,syl2tok_map):\n",
    "    #basically the first character in a token is always a boundary (following a boundary), the non-firsts are not.\n",
    "    ngram_list=ngram_line.split(',')\n",
    "    #tones=ngram_list[-4]\n",
    "    fname=ngram_list[-2].split('/')[1].split('_')[0]\n",
    "    syl_ids=ngram_list[-1].split('_')\n",
    "    txt_file='text_cmn/data/'+fname+'.txt'\n",
    "    tokenized_sentence=open(txt_file,'r').read()\n",
    "    tokenized_sentence=codecs.decode(tokenized_sentence,'utf-8')\n",
    "    untok_sentence=tokenized_sentence.replace(' ','')\n",
    "    #print tokenized_sentence\n",
    "    #print untok_sentence\n",
    "    decisions=[]\n",
    "    #print syl_ids\n",
    "    for syl in syl_ids:\n",
    "        #print syl\n",
    "        tok=tokenized_sentence.split(\" \")[int(syl2tok_map[int(syl)])]\n",
    "        #print untok_sentence[int(syl)]\n",
    "        #print tok\n",
    "        if untok_sentence[int(syl)]==tok[0]:\n",
    "            bound=1\n",
    "        else:\n",
    "            bound=0\n",
    "        decisions.append(bound)\n",
    "        \n",
    "    return decisions\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def is_singleton(coref_values):\n",
    "    for c in coref_values:\n",
    "        if c!='_':\n",
    "            return 1\n",
    "    return 0\n",
    "    \n",
    "def feature_extractor(ngram_line, N):\n",
    "    # N as in N-gram\n",
    "    # for bigram we'll have pos1 and pos2, for trigram, pos1, 2, 3; so basically we have a variable number of features\n",
    "    # depending on N\n",
    "    conll_indexes={'coref':-1,'func':-2,'head':-3,'entity':-4,'pos':-5,'text':3,'sent_id':0,'tok_id_spk':1,\\\n",
    "                   'tok_id_sent':2}\n",
    "\n",
    "    target_toks, syl2tok_map = map_to_conll(ngram_line)\n",
    "    assert len(target_toks)==N\n",
    "    \n",
    "    \n",
    "    #tok_bound1 means if tok 1 is following a word boundary or is it word-internal(0)\n",
    "    trigram_features='pos1,pos2,po3,func1,func2,func3,starting_pitch,ending_pitch,sent_position,is_entity,tok_bound1,\\\n",
    "    tok_bound2,tok_bound3,singleton'\n",
    "        \n",
    "    bigram_features='pos1,pos2,func1,func2,starting_pitch,ending_pitch,sent_position,is_entity,tok_bound1,tok_bound2,\\\n",
    "    singleton'\n",
    "    #for now we've ommitted - has_antecedent,has_anaphor,\n",
    "    pos_features=[i[conll_indexes['pos']] for i in target_toks]\n",
    "    func_features=[i[conll_indexes['func']] for i in target_toks]\n",
    "    start_pitch,end_pitch=get_pitch_features(ngram_line)\n",
    "    sent_position=float(target_toks[0][int(conll_indexes['tok_id_sent'])])/max(syl2tok_map.keys())\n",
    "    is_ngram_entity=int(is_entity([i[conll_indexes['entity']] for i in target_toks]))\n",
    "    is_bound=is_tok_bound(ngram_line,syl2tok_map)#this is a group feature, a list of features\n",
    "    #tok_id_spk_list=[i[conll_indexes['tok_id_spk']] for i in target_toks]\n",
    "    is_ngram_singleton=is_singleton([i[conll_indexes['coref']] for i in target_toks])#we'll collapse into one singleton feature\n",
    "    \n",
    "    features=[]\n",
    "    features.extend(pos_features)\n",
    "    features.extend(func_features)\n",
    "    features.extend([start_pitch,end_pitch,sent_position,is_ngram_entity])\n",
    "    features.extend(is_bound)\n",
    "    features.append(is_ngram_singleton)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngram='-0.439,-0.43,-0.422,-0.413,-0.405,-0.396,-0.387,-0.379,-0.37,-0.362,-0.353,-0.345,-0.336,-0.328,-0.319,-0.311,-0.302,-0.294,-0.285,-0.277,-0.268,-0.26,-0.251,-0.243,-0.234,-0.226,-0.217,-0.209,-0.2,-0.192,-0.184,-0.175,-0.167,-0.158,-0.15,-0.141,-0.133,-0.124,-0.116,-0.107,-0.099,-0.09,-0.082,-0.073,-0.065,-0.057,-0.048,-0.04,-0.031,-0.023,-0.014,-0.006,0.003,0.011,0.019,0.032,0.045,0.057,0.07,0.083,0.095,0.108,0.098,0.083,0.065,0.041,0.023,0.003,-0.031,-0.038,-0.043,-0.047,-0.054,-0.072,-0.081,-0.092,-0.1,-0.106,-0.136,-0.149,-0.163,-0.172,-0.178,-0.183,-0.181,-0.177,-0.178,-0.179,-0.18,-0.181,-0.181,-0.182,-0.183,-0.184,-0.185,-0.186,-0.187,-0.188,-0.188,-0.189,-0.19,-0.191,-0.192,-0.193,-0.194,-0.195,-0.195,-0.196,-0.197,-0.198,-0.199,-0.2,-0.2,-0.201,-0.202,-0.203,-0.204,-0.205,-0.206,-0.207,-0.208,-0.208,-0.209,-0.21,-0.211,-0.212,-0.213,-0.214,-0.215,-0.215,-0.216,-0.217,-0.218,-0.219,-0.22,-0.221,-0.222,-0.222,-0.223,-0.224,-0.225,-0.226,-0.227,-0.227,-0.228,-0.229,-0.23,-0.231,-0.232,-0.233,-0.234,-0.234,-0.235,-0.236,-0.237,-0.238,-0.239,-0.24,-0.241,-0.242,-0.242,-0.243,-0.244,-0.245,-0.246,-0.247,-0.248,-0.249,-0.249,-0.25,-0.251,-0.252,-0.253,-0.254,-0.255,-0.256,-0.258,-0.259,-0.259,-0.254,-0.289,-0.288,-0.287,-0.283,-0.315,-0.314,-0.341,-0.339,-0.339,-0.339,-0.379,-0.378,-0.378,-0.378,-0.447,-0.446,-0.446,-0.445,-0.447,-0.449,134,mid_mid_mid,syl_csv_norm_whole_meta/CHJ000043_whole,14_15_16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'compound:nn',\n",
       " 'compound:nn',\n",
       " 'dobj',\n",
       " '-0.439',\n",
       " '-0.449',\n",
       " 0.5882352941176471,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#demo\n",
    "feature_extractor(ngram,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next steps\n",
    "\n",
    "1. we have written the feature extractor working on one ngram line. \n",
    "2. when you have a ngrams csv file, simply feed it all lines, proided you have already generated all merged conll files for all speakers. Now we just have CHJ.\n",
    "3. then we'll just write to a feature file - training data, and we need to have class information also labelled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mdownsample_syl_3_meta_200_134_pkl_part_class.csv\u001b[m\u001b[m*\r\n",
      "\u001b[1m\u001b[32mdownsample_syl_3_meta_200_314_pkl_part_class.csv\u001b[m\u001b[m*\r\n",
      "\u001b[1m\u001b[32mdownsample_syl_3_meta_200_322_pkl_part_class.csv\u001b[m\u001b[m*\r\n",
      "\u001b[1m\u001b[32mdownsample_syl_3_meta_200_342_pkl_part_class.csv\u001b[m\u001b[m*\r\n",
      "\u001b[1m\u001b[32mdownsample_syl_3_meta_200_421_pkl_part_class.csv\u001b[m\u001b[m*\r\n"
     ]
    }
   ],
   "source": [
    "ls shape_class_csv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.405,-0.392,-0.379,-0.366,-0.354,-0.341,-0.328,-0.315,-0.302,-0.289,-0.276,-0.264,-0.251,-0.238,-0.225,-0.212,-0.199,-0.198,-0.196,-0.193,-0.189,-0.187,-0.186,-0.184,-0.181,-0.176,-0.169,-0.165,-0.161,-0.159,-0.156,-0.153,-0.15,-0.146,-0.143,-0.14,-0.137,-0.135,-0.134,-0.135,-0.135,-0.134,-0.132,-0.129,-0.127,-0.126,-0.128,-0.13,-0.132,-0.133,-0.132,-0.132,-0.132,-0.178,-0.179,-0.179,-0.177,-0.202,-0.201,-0.199,-0.199,-0.199,-0.2,-0.201,-0.201,-0.2,-0.196,-0.222,-0.258,-0.255,-0.248,-0.283,-0.282,-0.281,-0.28,-0.315,-0.314,-0.312,-0.311,-0.343,-0.341,-0.339,-0.339,-0.338,-0.308,-0.316,-0.34,-0.364,-0.389,-0.405,-0.406,-0.407,-0.408,-0.409,-0.41,-0.412,-0.42,-0.426,-0.427,-0.427,-0.427,-0.427,-0.427,-0.427,-0.426,-0.425,-0.425,-0.424,-0.424,-0.424,-0.425,-0.425,-0.426,-0.425,-0.425,-0.424,-0.424,-0.425,-0.426,-0.428,-0.43,-0.432,-0.435,-0.435,-0.434,-0.432,-0.43,-0.43,-0.432,-0.435,-0.436,-0.436,-0.436,-0.435,-0.434,-0.433,-0.431,-0.43,-0.429,-0.429,-0.429,-0.429,-0.43,-0.432,-0.439,-0.444,-0.438,-0.417,-0.397,-0.376,-0.355,-0.328,-0.3,-0.273,-0.245,-0.218,-0.191,-0.163,-0.136,-0.108,-0.081,-0.054,-0.027,0.001,0.028,0.055,0.082,0.11,0.137,0.164,0.191,0.218,0.245,0.272,0.299,0.326,0.353,0.38,0.407,0.434,0.461,0.488,0.514,0.533,0.524,0.464,0.454,0.444,0.429,0.399,0.353,0.315,0.281,0.246,0.209,0.197,0.197,-0.207,-0.201,-0.194,134,mid_mid_mid,syl_csv_norm_whole_meta/CHJ000008_whole,12_13_14,0\n"
     ]
    }
   ],
   "source": [
    "data134=open('shape_class_csv/downsample_syl_3_meta_200_134_pkl_part_class.csv','r').read().split('\\n')\n",
    "print data134[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "def get_data_label(ngram_file):\n",
    "    lines=open(ngram_file,'r').read().split('\\n')\n",
    "    all_data,all_label=[],[]\n",
    "    for l in lines:\n",
    "        if l=='':\n",
    "            continue\n",
    "        line=l.split(',')\n",
    "        data=line[:-1]\n",
    "        label=line[-1]\n",
    "        all_data.append(data)\n",
    "        all_label.append(label)\n",
    "    return all_data,all_label\n",
    "\n",
    "def build_train(ngram_file):\n",
    "    \"\"\"given one ngram_file, return a matrix of features and including label\"\"\"\n",
    "    all_data,all_label=get_data_label(ngram_file)\n",
    "    all_features=[]\n",
    "    for i in range(1,len(all_data)):\n",
    "        data=all_data[i]\n",
    "        label=all_label[i]\n",
    "        ngram_line=','.join(data)\n",
    "        #print ngram_line[-4:]\n",
    "        features=feature_extractor(ngram_line,3)\n",
    "        features.append(label)\n",
    "        all_features.append(features)\n",
    "    return all_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f='shape_class_csv/downsample_syl_3_meta_200_134_pkl_part_class.csv'\n",
    "train_data=build_train(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data,all_label=get_data_label(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['134', 'mid_mid_mid', 'syl_csv_norm_whole_meta/CHJ000043_whole', '14_15_16']\n"
     ]
    }
   ],
   "source": [
    "print all_data[3][-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trigram_features='pos1,pos2,po3,func1,func2,func3,starting_pitch,ending_pitch,sent_position,is_entity,tok_bound1,tok_bound2,tok_bound3,singleton,label'\n",
    "tf=trigram_features.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "f='shape_class_csv/downsample_syl_3_meta_200_134_pkl_part_class.csv'\n",
    "N=int(f.split('downsample_syl_')[1][0])\n",
    "print N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['pos1',\n",
       "  'pos2',\n",
       "  'po3',\n",
       "  'func1',\n",
       "  'func2',\n",
       "  'func3',\n",
       "  'starting_pitch',\n",
       "  'ending_pitch',\n",
       "  'sent_position',\n",
       "  'is_entity',\n",
       "  'tok_bound1',\n",
       "  'tok_bound2',\n",
       "  'tok_bound3',\n",
       "  'singleton',\n",
       "  'label']]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AD', 'AD', 'VA', 'advmod', 'advmod', 'ROOT', '-0.063', '-0.116', 0.5714285714285714, 0, 1, 1, 1, 0, '0']\n"
     ]
    }
   ],
   "source": [
    "print train_data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "outcsv='try.csv'\n",
    "with open(outcsv, 'wb') as f:\n",
    "    csv.writer(f).writerow(tf)\n",
    "    csv.writer(f).writerows(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we found a mistake: some sentence segmentation errors by stanford parser, so that sentence file names (.txt) cannot map correctly to sentID in conll. To remedy this problem, we found the inconsistent number of sentences in conll with true file numbers, then we re-built data with one extra newline between sentences (this error was caused by .\\ntok being parsed as one token). \n",
    "\n",
    "# after we re-run these, we need to re-verify that they are consistent.(all of these are done on macbook Pro) then, we can re-run the feature extractor here and see what happens.\n",
    "\n",
    "# THIS ERROR IS NOW CORRECTED.[5/19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_coref_chains(conll_merged_file):\n",
    "    f=open(conll_merged_file,'r').read().split('\\n')\n",
    "    all_coref=[]\n",
    "    for line in f:\n",
    "        if line.strip().startswith('#'):\n",
    "            continue\n",
    "        if line.strip()!='':\n",
    "            l=line.strip().split('\\t')\n",
    "            all_coref.append(l[-1])\n",
    "    return all_coref\n",
    "    \n",
    "\n",
    "    \n",
    "def has_antecedent(ngram_line,tok_id_spk_list):    \n",
    "    #tok_id_spk_list is the list of tok_id_spk for the N tokens under consideration\n",
    "    ngram_list=ngram_line.split(',')\n",
    "    fname=ngram_list[-2].split('/')[1].split('_')[0]\n",
    "    conll_merged_file=fname[:3]+'_merge.conll'\n",
    "    coref_chain=extract_coref_chains(conll_merged_file)\n",
    "    for tok_id_spk in tok_id_spk_list:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map from shape csv to .phons file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram='-0.405,-0.392,-0.379,-0.366,-0.354,-0.341,-0.328,-0.315,-0.302,-0.289,-0.276,-0.264,-0.251,-0.238,-0.225,-0.212,-0.199,-0.198,-0.196,-0.193,-0.189,-0.187,-0.186,-0.184,-0.181,-0.176,-0.169,-0.165,-0.161,-0.159,-0.156,-0.153,-0.15,-0.146,-0.143,-0.14,-0.137,-0.135,-0.134,-0.135,-0.135,-0.134,-0.132,-0.129,-0.127,-0.126,-0.128,-0.13,-0.132,-0.133,-0.132,-0.132,-0.132,-0.178,-0.179,-0.179,-0.177,-0.202,-0.201,-0.199,-0.199,-0.199,-0.2,-0.201,-0.201,-0.2,-0.196,-0.222,-0.258,-0.255,-0.248,-0.283,-0.282,-0.281,-0.28,-0.315,-0.314,-0.312,-0.311,-0.343,-0.341,-0.339,-0.339,-0.338,-0.308,-0.316,-0.34,-0.364,-0.389,-0.405,-0.406,-0.407,-0.408,-0.409,-0.41,-0.412,-0.42,-0.426,-0.427,-0.427,-0.427,-0.427,-0.427,-0.427,-0.426,-0.425,-0.425,-0.424,-0.424,-0.424,-0.425,-0.425,-0.426,-0.425,-0.425,-0.424,-0.424,-0.425,-0.426,-0.428,-0.43,-0.432,-0.435,-0.435,-0.434,-0.432,-0.43,-0.43,-0.432,-0.435,-0.436,-0.436,-0.436,-0.435,-0.434,-0.433,-0.431,-0.43,-0.429,-0.429,-0.429,-0.429,-0.43,-0.432,-0.439,-0.444,-0.438,-0.417,-0.397,-0.376,-0.355,-0.328,-0.3,-0.273,-0.245,-0.218,-0.191,-0.163,-0.136,-0.108,-0.081,-0.054,-0.027,0.001,0.028,0.055,0.082,0.11,0.137,0.164,0.191,0.218,0.245,0.272,0.299,0.326,0.353,0.38,0.407,0.434,0.461,0.488,0.514,0.533,0.524,0.464,0.454,0.444,0.429,0.399,0.353,0.315,0.281,0.246,0.209,0.197,0.197,-0.207,-0.201,-0.194,134,mid_mid_mid,syl_csv_norm_whole_meta/CHJ000008_whole,12_13_14'\n",
    "ngram_list=ngram.split(',')\n",
    "tones=ngram_list[-4]\n",
    "fname=ngram_list[-2].split('/')[1].split('_')[0]\n",
    "syl_ids=ngram_list[-1].split('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHJ000008'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phons_path='text_cmn/data/'\n",
    "phons_file=fname+'.phons'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_phons_file(phons_file):\n",
    "    \"\"\"read a phons file and return a list of vowels that carry tone marks\"\"\"\n",
    "    all_lines=open(phons_file,'r').read().split('\\n')\n",
    "    all_nuclear=[]\n",
    "    for l in all_lines:\n",
    "        if l!=\"\":\n",
    "            line=l.split(' ')\n",
    "            m = re.search(r'\\d+$', line[-1])\n",
    "            if m:\n",
    "                all_nuclear.append(line[-1])\n",
    "    return all_nuclear\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uo1 iu3 ii4\n"
     ]
    }
   ],
   "source": [
    "l=parse_phons_file(phons_path+phons_file)\n",
    "print l[int(syl_ids[0])],l[int(syl_ids[1])],l[int(syl_ids[2])]\n",
    "a=[l[int(i)][:-1] for i in syl_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_phons_feature(ngram,phons_path):\n",
    "    \"given a ngram line, extract phonological features\"\n",
    "    #features: nuclear of all vowels in the ngram; (2 or 3)\n",
    "    #features: previous and following tone mark of the ngram; (2)\n",
    "    ngram_list=ngram.split(',')\n",
    "    tones=ngram_list[-4]\n",
    "    fname=ngram_list[-2].split('/')[1].split('_')[0]\n",
    "    syl_ids=ngram_list[-1].split('_')\n",
    "    #phons_path='text_cmn/data/'\n",
    "    phons_file=fname+'.phons'\n",
    "    all_nuclear=parse_phons_file(phons_path+phons_file)\n",
    "    #this is a list of all nuclears\n",
    "    vowel_features=[all_nuclear[int(i)][:-1] for i in syl_ids]\n",
    "    #print vowel_features\n",
    "    prev_tone=all_nuclear[int(syl_ids[0])-1][-1]\n",
    "    next_tone=all_nuclear[int(syl_ids[0])+1][-1]\n",
    "    vowel_features.extend([prev_tone,next_tone])\n",
    "    return vowel_features\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uo', 'iu', 'ii', '4', '3']\n"
     ]
    }
   ],
   "source": [
    "print extract_phons_feature(ngram,phons_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a dictionary for collapsing tagset POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagset_dict={'VADJ':['VA','VC','VE','VV'],'NOUN':['NR','NT','NN','PN'],'DETN':['DT','CD','OD'],\\\n",
    "            'OTRO':['LC','M','P','CC','CS','DEC','DEG','DER','DEV','SP','AS','ETC','MSP',   \\\n",
    "                   'IJ','ON','PU','JJ','FW','LB','SB','BA'], 'AD':['AD']}\n",
    "rdict=reverse_dict(tagset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reverse_dict(tag_dict):\n",
    "    rdict={}\n",
    "    for k,v in tag_dict.iteritems():\n",
    "        for i in v:\n",
    "            rdict[i]=k\n",
    "    return rdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdict=reverse_dict(tagset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vowel features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#is_nasal,is_dipthong,is_high,is_low,is_front,is_back,is_round\n",
    "\n",
    "def is_nasal(nuclear):\n",
    "    if 'n' in nuclear:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_high(x):\n",
    "    high=set(['i','v','u'])\n",
    "    for i in x:\n",
    "        if i in high:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_low(x):\n",
    "    low=set(['a'])\n",
    "    for i in x:\n",
    "        if i in low:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_dipthong(x):\n",
    "    f=set(['a','i','e','v','u','o'])\n",
    "    for i in range(len(x)-1):\n",
    "        if x[i] in f:\n",
    "            if x[i+1] in f and x[i]!=x[i+1]:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_front(x):\n",
    "    f=set(['a','i','e','v'])\n",
    "    for i in x:\n",
    "        if i in f:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_back(x):\n",
    "    f=set(['u','o'])\n",
    "    for i in x:\n",
    "        if i in f:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def is_round(x):\n",
    "    f=set(['o','u','v'])\n",
    "    for i in x:\n",
    "        if i in f:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_dipthong('ii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
