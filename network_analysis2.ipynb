{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "%matplotlib inline\n",
    "from collections import defaultdict\n",
    "import pylab as plt\n",
    "from os import listdir\n",
    "\n",
    "def get_clusters(part):\n",
    "    cluster_assignment=defaultdict(list)\n",
    "    for k,v in part.iteritems():\n",
    "        cluster_assignment[v].append(k)\n",
    "    #print out the length of each cluster\n",
    "    for k in cluster_assignment.keys():\n",
    "        print k,len(cluster_assignment[k])\n",
    "    return cluster_assignment\n",
    "\n",
    "\n",
    "\n",
    "def read_csv_data_meta_unigram(data_file):\n",
    "    f=open(data_file,'r').read().split('\\n')\n",
    "    all_csv_data=[]\n",
    "    all_lab=[]\n",
    "    for line in f:\n",
    "        if line=='':\n",
    "            continue\n",
    "        l=line.split(',')[:-1]\n",
    "        b=line.split(',')[-1]\n",
    "        #l=[float(i) for i in l]\n",
    "        all_csv_data.append(l)\n",
    "        all_lab.append(b)\n",
    "    return all_csv_data,all_lab\n",
    "\n",
    "\n",
    "def read_csv_data_meta(data_file):\n",
    "    f=open(data_file,'r').read().split('\\n')\n",
    "    all_csv_data=[]\n",
    "    all_lab=[]\n",
    "    for line in f:\n",
    "        if line=='':\n",
    "            continue\n",
    "        l=line.split(',')[:-4]\n",
    "        b=line.split(',')[-4:]\n",
    "        #l=[float(i) for i in l]\n",
    "        all_csv_data.append(l)\n",
    "        all_lab.append(b)\n",
    "    return all_csv_data,all_lab\n",
    "\n",
    "\n",
    "def plot_clusters(path,pfile,csv_file):\n",
    "    #pfile='downsample_syl_3_meta_200_421_pkl_part.pkl'\n",
    "    #path='core_subdata/'\n",
    "    part=pickle.load(open(path+pfile,'r'))\n",
    "    clust=get_clusters(part)\n",
    "    #csv_file='subdata/downsample_syl_3_meta_200_421.csv'\n",
    "    ngram=csv_file.split('.')[0].split('_')[-1]\n",
    "    csvdata,lab=read_csv_data_meta(csv_file)\n",
    "    #print len(csvdata)\n",
    "    for j in clust.keys():\n",
    "        inds=clust[j]\n",
    "        plt.figure()\n",
    "        for i in inds:\n",
    "            #print 'i',i\n",
    "            \n",
    "            #print len(csvdata[i])\n",
    "            plt.plot(csvdata[i])\n",
    "            plt.title(ngram+\"|\"+str(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_classes(path,pfile,csv_file):\n",
    "    part=pickle.load(open(path+pfile,'r'))\n",
    "    clust=get_clusters(part)\n",
    "    ngram=csv_file.split('.')[0].split('_')[-1]\n",
    "    csvdata,lab=read_csv_data_meta(csv_file)\n",
    "    all_data=[]\n",
    "    for key in clust.keys():\n",
    "        inds=clust[key]\n",
    "        if len(inds)<50:\n",
    "            print 'key excluded:',key\n",
    "            continue\n",
    "        for i in inds:\n",
    "            data=csvdata[i]\n",
    "            labels=lab[i]\n",
    "            data.extend(labels)\n",
    "            data.append(str(key))\n",
    "            all_data.append(data)\n",
    "    return all_data\n",
    "            \n",
    "            \n",
    "def write_file(data,pfile):\n",
    "    outfile=pfile.replace('.pkl','_class.csv')\n",
    "    f=open(outfile,'w').close()\n",
    "    f=open(outfile,'a')\n",
    "    num_col=len(data[0])\n",
    "    header=[str(i+1) for i in range(num_col-1)]\n",
    "    header.append('label')\n",
    "    f.write(','.join(header)+'\\n')\n",
    "    for line in data:\n",
    "        f.write(','.join(line)+'\\n')\n",
    "    f.close()         \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# write to a csv file where each line has a label indicating the class the the shape of the ts belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path='community_partition/core_subdata/'\n",
    "pfiles = [ f for f in listdir(path) if f.endswith('_part.pkl')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 181\n",
      "1 183\n",
      "2 212\n",
      "3 116\n",
      "4 2\n",
      "5 4\n",
      "6 9\n",
      "7 5\n",
      "8 2\n",
      "9 12\n",
      "10 12\n",
      "11 2\n",
      "12 2\n",
      "13 11\n",
      "14 4\n",
      "15 2\n",
      "16 2\n",
      "17 6\n",
      "18 2\n",
      "19 3\n",
      "20 2\n",
      "21 2\n",
      "22 2\n",
      "23 2\n",
      "24 5\n",
      "25 2\n",
      "key excluded: 4\n",
      "key excluded: 5\n",
      "key excluded: 6\n",
      "key excluded: 7\n",
      "key excluded: 8\n",
      "key excluded: 11\n",
      "key excluded: 12\n",
      "key excluded: 14\n",
      "key excluded: 15\n",
      "key excluded: 16\n",
      "key excluded: 17\n",
      "key excluded: 18\n",
      "key excluded: 19\n",
      "key excluded: 20\n",
      "key excluded: 21\n",
      "key excluded: 22\n",
      "key excluded: 23\n",
      "key excluded: 24\n",
      "key excluded: 25\n"
     ]
    }
   ],
   "source": [
    "#demo\n",
    "pfile=pfiles[0]\n",
    "csv_file='subdata/'+pfile.replace('_pkl_part.pkl','.csv')\n",
    "d=get_classes(path,pfile,csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 181\n",
      "1 183\n",
      "2 212\n",
      "3 116\n",
      "4 2\n",
      "5 4\n",
      "6 9\n",
      "7 5\n",
      "8 2\n",
      "9 12\n",
      "10 12\n",
      "11 2\n",
      "12 2\n",
      "13 11\n",
      "14 4\n",
      "15 2\n",
      "16 2\n",
      "17 6\n",
      "18 2\n",
      "19 3\n",
      "20 2\n",
      "21 2\n",
      "22 2\n",
      "23 2\n",
      "24 5\n",
      "25 2\n",
      "key excluded: 4\n",
      "key excluded: 5\n",
      "key excluded: 6\n",
      "key excluded: 7\n",
      "key excluded: 8\n",
      "key excluded: 9\n",
      "key excluded: 10\n",
      "key excluded: 11\n",
      "key excluded: 12\n",
      "key excluded: 13\n",
      "key excluded: 14\n",
      "key excluded: 15\n",
      "key excluded: 16\n",
      "key excluded: 17\n",
      "key excluded: 18\n",
      "key excluded: 19\n",
      "key excluded: 20\n",
      "key excluded: 21\n",
      "key excluded: 22\n",
      "key excluded: 23\n",
      "key excluded: 24\n",
      "key excluded: 25\n",
      "0 85\n",
      "1 119\n",
      "2 154\n",
      "3 17\n",
      "4 168\n",
      "5 175\n",
      "6 11\n",
      "7 2\n",
      "8 11\n",
      "9 2\n",
      "10 3\n",
      "11 2\n",
      "12 4\n",
      "key excluded: 3\n",
      "key excluded: 6\n",
      "key excluded: 7\n",
      "key excluded: 8\n",
      "key excluded: 9\n",
      "key excluded: 10\n",
      "key excluded: 11\n",
      "key excluded: 12\n",
      "0 57\n",
      "1 53\n",
      "2 120\n",
      "3 112\n",
      "4 119\n",
      "5 80\n",
      "6 2\n",
      "7 3\n",
      "8 2\n",
      "9 11\n",
      "10 3\n",
      "11 3\n",
      "12 2\n",
      "key excluded: 6\n",
      "key excluded: 7\n",
      "key excluded: 8\n",
      "key excluded: 9\n",
      "key excluded: 10\n",
      "key excluded: 11\n",
      "key excluded: 12\n",
      "0 82\n",
      "1 136\n",
      "2 38\n",
      "3 173\n",
      "4 2\n",
      "5 58\n",
      "6 5\n",
      "7 172\n",
      "8 9\n",
      "9 12\n",
      "10 2\n",
      "11 6\n",
      "12 3\n",
      "13 3\n",
      "14 7\n",
      "15 2\n",
      "16 2\n",
      "17 2\n",
      "18 2\n",
      "19 4\n",
      "20 2\n",
      "21 2\n",
      "22 3\n",
      "23 3\n",
      "24 3\n",
      "25 2\n",
      "26 2\n",
      "27 2\n",
      "28 2\n",
      "29 2\n",
      "30 2\n",
      "31 2\n",
      "32 2\n",
      "33 2\n",
      "34 2\n",
      "key excluded: 2\n",
      "key excluded: 4\n",
      "key excluded: 6\n",
      "key excluded: 8\n",
      "key excluded: 9\n",
      "key excluded: 10\n",
      "key excluded: 11\n",
      "key excluded: 12\n",
      "key excluded: 13\n",
      "key excluded: 14\n",
      "key excluded: 15\n",
      "key excluded: 16\n",
      "key excluded: 17\n",
      "key excluded: 18\n",
      "key excluded: 19\n",
      "key excluded: 20\n",
      "key excluded: 21\n",
      "key excluded: 22\n",
      "key excluded: 23\n",
      "key excluded: 24\n",
      "key excluded: 25\n",
      "key excluded: 26\n",
      "key excluded: 27\n",
      "key excluded: 28\n",
      "key excluded: 29\n",
      "key excluded: 30\n",
      "key excluded: 31\n",
      "key excluded: 32\n",
      "key excluded: 33\n",
      "key excluded: 34\n",
      "0 85\n",
      "1 156\n",
      "2 148\n",
      "3 160\n",
      "4 183\n",
      "5 59\n",
      "6 120\n",
      "7 2\n",
      "8 3\n",
      "9 4\n",
      "10 2\n",
      "11 2\n",
      "12 3\n",
      "13 4\n",
      "14 2\n",
      "15 2\n",
      "16 2\n",
      "17 2\n",
      "18 2\n",
      "19 3\n",
      "20 2\n",
      "21 4\n",
      "22 2\n",
      "key excluded: 7\n",
      "key excluded: 8\n",
      "key excluded: 9\n",
      "key excluded: 10\n",
      "key excluded: 11\n",
      "key excluded: 12\n",
      "key excluded: 13\n",
      "key excluded: 14\n",
      "key excluded: 15\n",
      "key excluded: 16\n",
      "key excluded: 17\n",
      "key excluded: 18\n",
      "key excluded: 19\n",
      "key excluded: 20\n",
      "key excluded: 21\n",
      "key excluded: 22\n"
     ]
    }
   ],
   "source": [
    "#process all\n",
    "for pfile in pfiles:\n",
    "    csv_file='subdata/'+pfile.replace('_pkl_part.pkl','.csv')\n",
    "    d=get_classes(path,pfile,csv_file)\n",
    "    write_file(d,pfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data written to file\n",
    "\n",
    "note that the data will be smaller than the original csv file - some TS are excluded because some of the cluster keys are excluded, because they are too small a cluster - we consider them as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification using ts data\n",
    "\n",
    "we used ts ngram data to classify into these categories (classes). It is able to do it with RF in general with high accuracy between 85% to 90%. \n",
    "\n",
    "next we can proceed to inter-domain classification. you can include starting and ending pitch as a feature. but we want in general the out of domain features. for data, we need to map between the text and these TS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
